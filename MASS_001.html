<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Mass - Cache Concepts | unrealcode.net</title>
<meta name="description" content="Mass - Cache Concepts">
<meta charset="utf-8">
<link rel="stylesheet" href="./css/styles.css">
<link rel="stylesheet" href="./css/syntaxhighlightingstyles.css">
</head>
<body>
<div id="app" data-server-rendered="true">
<div class="theme-container">
</div>
<header class="navbar">
<div class="sidebar-button">
</div>
<a href="index.html" aria-current="page" class="home-link router-link-exact-active router-link-active">
<span class="site-name">
unrealcode.net
</span>
</a>
<div class="links">
<nav class="nav-links can-hide">
<div class="nav-item">
<a href="index.html" aria-current="page" class="nav-link router-link-exact-active router-link-active">
Home
</a>
</div>
</nav>
</div>
</header>
<div class="sidebar-mask">
</div>
<aside class="sidebar">
<ul class="sidebar-links">
<li>
<section class="sidebar-group depth-0">
<p class="sidebar-heading open">
<span>
Mass - Cache Concepts
</span>
</p>
<ul class="sidebar-links sidebar-group-items">
<li>
<a class="sidebar-link" href="#about-entity-component-systems">
About Entity Component Systems 
</a>
</li>
<li>
<a class="sidebar-link" href="#cpu-memory-caching">
CPU Memory Caching
</a>
</li>
<li>
<a class="sidebar-link" href="#cache-lines">
Cache Lines
</a>
</li>
<li>
<a class="sidebar-link" href="#entity-component-systems">
Entity Component Systems
</a>
</li>
<li>
<a class="sidebar-link" href="#a-simple-example">
A simple example 
</a>
</li>
<li>
<a class="sidebar-link" href="#data-types-and-sizes">
Data Types and Sizes
</a>
</li>
<li>
<a class="sidebar-link" href="#terminology">
Terminology
</a>
</li>
<li>
<a class="sidebar-link" href="#threading-and-false-sharing">
Threading and False Sharing
</a>
</li>
<li>
<a class="sidebar-link" href="#references">
References
</a>
</li>
</ul>
</section>
</li>
</ul>
</aside>
<main class="page">
<div class="theme-default-content content__default">
<h1> Mass - Cache Concepts</h1>
<h2 id="about-entity-component-systems">About Entity Component Systems </h2>
<p>This is the first of a series of articles documenting what I know about the Unreal Mass framework.</p>
<p>Mass is an Unreal Engine module which implements an <a href="https://en.wikipedia.org/wiki/Entity_component_system">Entity Component System</a> or ECS.  </p>
<p>An entity component system is an architectural pattern where data related to objects, such as their velocity or position, is stored in a specific
way in memory to speed up access to that data.  The way an entity component system stores data in memory is influenced by the memory caching architecture of the CPU it is running on.</p>
<h2 id="cpu-memory-caching">CPU Memory Caching</h2>
<p>This is a very high-level view of caching.  When the CPU wants to operate on a value (say to increment it or to add it to another value) that value first must be copied from main memory to the CPU.</p>
<p>If there was no memory caching (like we are back in 1980), every time the CPU needed to access a value it would have to be copied 
from main memory. To avoid this CPUs have cache memory, which is a small amount of memory
physically either in the CPU or close by it.  When a value is required by the CPU, 
the CPU checks to see if the value is already in the cache - if it is then
it is copied from the cache to the CPU, if it is not it is copied from the
memory to the CPU and also stored in the cache so it might be found if it is
required again. </p>
<p>This image shows the cache layout on a Ryzen 8 core CPU:</p>
<p><img src="/images/Zen3_arch_20.jpg" alt="" /></p>
<p>From this we can see aspects of this specific memory architecture:</p>
<ul>
<li> there are 3 levels of cache L1, L2, L3 (some chips have more)</li>
<li> the L1 cache consists of two separate parts the I Cache (instruction) and the D Cache (data).  When considering 
entity component systems we only care about the data cache. </li>
<li> each level is larger than the previous one - L1 is 32K, L2 is 512K, L3 is 32M</li>
<li> each core has its own L1 and L2 cache</li>
<li> the L3 cache is shared between all cores</li>
</ul>
<p>When a value is required by the CPU:</p>
<ul>
<li> if it is in the L1 cache it is copied from L1, otherwise</li>
<li> if it is in the L2 cache it is copied from L2, otherwise</li>
<li> if it is in the L3 cache it is copied from L3, otherwise</li>
<li> it is copied from memory</li>
</ul>
<p>Each successive level of cache is slower to access than the previous one - it has a higher latency.  To copy a value
(ignoring the size of the value for now) to the CPU takes increasing time.  Note that the timings vary 
from system to system depending on CPU type and memory speed, but the general trend is the same.</p>
<p>This image shows values for a 12 core Ryzen CPU:</p>
<p><img src="/images/mass_001_002.PNG" alt="" /></p>
<p>Fetching data from:</p>
<ul>
<li> L1 cache to CPU register - 4 cycles - 0.8ns</li>
<li> L2 cache to CPU register - 12 cycles - 2.4ns</li>
<li> L3 cache to CPU register - 52 cycles - 10.6ns</li>
<li> memory to CPU register - 52.7 ns</li>
</ul>
<p>So comparatively, fetching data from:</p>
<ul>
<li> L2 cache takes 3 times as long as from L1</li>
<li> L3 cache takes 13 times as long as from L1</li>
<li> memory takes 66 times as long as from L1</li>
</ul>
<p>So in the extreme case a calculation could be done 66 times faster if all the required data was in the L1 cache
as opposed to only being in memory.</p>
<h2 id="cache-lines">Cache Lines</h2>
<p>Values are not copied from main memory to cache memory one at a time.  They are
copied in cache lines.  The size of a cache line depends on the CPU model but is typically 64 bytes.  The CoreInfo
program listed in references at the end of this page can be used to print the cache line size for your CPU.</p>
<p>Assuming a cache line size of 64 bytes, and an integer size of 4 bytes, one cache line can fit 16 integer
values - this means that ideally we want to write a program so that each time we copy values from 
main memory into the L1 cache we move 16 integer values at a time.  The first time we 
need one of the values we pay the 52.7ns cost of copying a cache line from main memory to the 
L1 cache, but for the remaining 15 integer values we only pay the 0.8ns for each value.</p>
<h2 id="entity-component-systems">Entity Component Systems</h2>
<p>An entity component system tries to group data which will be used into contiguous arrays so
that every value used in a calculation is on a cache line with other values which will also
be used, and also that no data is copied from main memory to the CPU which is not used.</p>
<h2 id="a-simple-example">A simple example </h2>
<p>Say we have a Robot class and each instance of the Robot class robot has a position, a velocity, 
a size and an active flag like this:</p>
<div class="cplusplus"><pre>
<span class="keyword">struct</span> FRobot
{
	FVector Position;   <span class="comment">// 3x8 = 24 bytes</span>
	FVector Velocity;   <span class="comment">// 3x8 = 24 bytes</span>
	FVector Size;       <span class="comment">// 3x8 = 24 bytes</span>
	<span class="keyword">bool</span> bActive;       <span class="comment">// 1 byte</span>
};
</pre></div>
<p>We have a array of 1 million Robots, and we want to set the bActive property to true on every tick event.  This code will do that:</p>
<div class="cplusplus"><pre>
<span class="keyword">namespace</span>
{
	<span class="keyword">struct</span> FRobot
	{
		FVector Position;   <span class="comment">// 3x8 = 24 bytes</span>
		FVector Velocity;   <span class="comment">// 3x8 = 24 bytes</span>
		FVector Size;       <span class="comment">// 3x8 = 24 bytes</span>
		<span class="keyword">bool</span> bActive;
	};

	TArray&lt;FRobot&gt; RobotArray;

	<span class="keyword">void</span> ActivateRobots()
	{
		TRACE_CPUPROFILER_EVENT_SCOPE(ActivateRobots)
		{
			<span class="keyword">for</span> (FRobot&amp; Robot : RobotArray)
			{
				Robot.bActive = <span class="keyword">true</span>;
			}
		}
	}
}

<span class="keyword">void</span> AMyCharacter::BeginPlay()
{
	Super::BeginPlay();
	<span class="comment">// initialize array</span>
	size_t N = 1024 * 1024;
	RobotArray.AddDefaulted(N);
}

<span class="keyword">void</span> AMyCharacter::Tick(<span class="keyword">float</span> DeltaTime)
{
	Super::Tick(DeltaTime);
	ActivateRobots();
}

</pre></div>
<p>Using Unreal Insights we can see the ActivateRobots call takes 7.38ms (2400ms for 325 calls):</p>
<p><img src="/images/mass_001_004.PNG" alt="" /></p>
<p>If we restructured the code to save each Robot property in a separate array, so we have one
array for Ages, one for Positions etc. we have this:</p>
<div class="cplusplus"><pre>
<span class="keyword">namespace</span> {

	<span class="keyword">struct</span> FRobots
	{
		TArray&lt;FVector&gt; Positions;
		TArray&lt;FVector&gt; Velocities;
		TArray&lt;FVector&gt; Sizes;
		TArray&lt;<span class="keyword">bool</span>&gt; Actives;

		<span class="keyword">void</span> AddDefaulted(<span class="keyword">const</span> size_t N)
		{
			Positions.AddDefaulted(N);
			Velocities.AddDefaulted(N);
			Sizes.AddDefaulted(N);
			Actives.AddDefaulted(N);
		}
	};

	FRobots Robots;

	<span class="keyword">void</span> ActivateRobots_SOA()
	{
		TRACE_CPUPROFILER_EVENT_SCOPE(ActivateRobots_SOA)
		{
			<span class="keyword">for</span>( <span class="keyword">bool</span>&amp; bActive : Robots.Actives)
			{
				bActive = <span class="keyword">true</span>;
			}
		}
	}
}

<span class="keyword">void</span> AMyCharacter::BeginPlay()
{
	Super::BeginPlay();
	size_t N = 1024 * 1024;
	Robots.AddDefaulted(N);
}

<span class="keyword">void</span> AMyCharacter::Tick(<span class="keyword">float</span> DeltaTime)
{
	Super::Tick(DeltaTime);
	ActivateRobots_SOA(Robots);
}
</pre></div>
<p>Here the _SOA appended to the name to distinguish the two approaches
refers to Structure-of-Arrays.  This terminology comes from the fact that the first approach uses an Array-of-Structures, the
second approach as a single structure which contains the data in arrays.</p>
<p>Using Unreal Insights we can see the ActivateRobots_SOA call takes 1.18ms (385ms for 325 calls):</p>
<p><img src="/images/mass_001_006.PNG" alt="" /></p>
<p>So by changing to storing the data in arrays we have made the function  go 6 times faster.  This
is what the Mass entity component system tries to do for objects in Unreal Engine, it 
enables support for much larger numbers of objects than would be support by a strictly
object oriented approach.</p>
<h2 id="data-types-and-sizes">Data Types and Sizes</h2>
<p>Storing data in arrays not only increases performance but it also saves memory:  </p>
<div class="cplusplus"><pre>
<span class="keyword">struct</span> FRobot
{
	FVector Position;   <span class="comment">// 3x8 = 24 bytes</span>
	FVector Velocity;   <span class="comment">// 3x8 = 24 bytes</span>
	FVector Size;       <span class="comment">// 3x8 = 24 bytes</span>
	<span class="keyword">bool</span> bActive;
};
</pre></div>
<p>When an array is filled with FRobot objects they align according to c++ alignment rules.  The alignment 
of a struct is usually the maximum alignment of each of its members, so in this case  the alignment 
alignof(FRobot) is alignof(FVector) which is 24 bytes - so we waste 23 bytes for each
FRobot object.</p>
<p>Changing the bool to an int32 type has an observable effect:</p>
<p><img src="/images/mass_001_007.PNG" alt="" /></p>
<p>The ActivateRobots call changes takes 7.41ms (2500ms for 337 calls), an insignificant change from 7.38.</p>
<p>The ActivateRobots_SOA call changes takes 0.87ms (295ms for 337 calls), an 26% improvement from 1.18ms.  This
is probably due to the int32 property having better alignment than the single byte bool property.</p>
<h2 id="terminology">Terminology</h2>
<p>&quot;cache locality&quot; refers to the position of one thing with regard to another.  If an algorithm
uses two variables it is good if they are near each other in memory so when the first
one is copied from main memory to a cache line, the second one will also be in 
the same cache line and will be copied as well.  So when it is needed by the CPU
it will already be in the cache.</p>
<h2 id="threading-and-false-sharing">Threading and False Sharing</h2>
<p>Cache lines need to be considered when using threading to speed up tasks. </p>
<p>We can create a background task which counts the  number of active robots like so:</p>
<div class="cplusplus"><pre>
<span class="keyword">class</span> FCountActiveRobotsTask : <span class="keyword">public</span> FNonAbandonableTask
{
<span class="keyword">public</span>:
	FCountActiveRobotsTask(<span class="keyword">const</span> int32 InStart, <span class="keyword">const</span> int32 InNum, int32&amp; InCount )
		: Start(InStart), Num(InNum), Count(InCount)
	{
	}

	<span class="keyword">void</span> DoWork()
	{
		<span class="keyword">for</span> (<span class="keyword">int</span> i = Start; i &lt; Start + Num; ++i)
		{
			<span class="keyword">if</span> (Robots.Actives[i])
			{
				Count++;
			}
		}
	}

	TStatId GetStatId() <span class="keyword">const</span> { <span class="keyword">return</span> TStatId(); }

<span class="keyword">private</span>:
	int32 Start;
	int32 Num;
	int32&amp; Count;
};
</pre></div>
<p>We launch this background task for one thread only like this:</p>
<div class="cplusplus"><pre>
<span class="keyword">void</span> Count_Robots_Background_Thread()
{
	int32 Count = 0;
	TRACE_CPUPROFILER_EVENT_SCOPE(Count_Robots_Background_Thread)
	{
		<span class="keyword">auto</span> Task = 
		  MakeUnique&lt;FAsyncTask&lt;FCountActiveRobotsTask&gt;&gt;( 0, Robots.Actives.Num(), Count );
		<span class="keyword">if</span> (Task)
		{
			Task-&gt;StartBackgroundTask();
			Task-&gt;EnsureCompletion(<span class="keyword">true</span>, <span class="keyword">true</span>);
		}
	}
}
</pre></div>
<p>Comparing running on the main thread with doing the same task on a single background thread we see these
timings:</p>
<p><img src="/images/mass_001_008.PNG" alt="" /></p>
<p>Running on the background thread is very slightly slower because of the overhead if starting and managing the thread.</p>
<p>We can change from running on one background thread to four like this:</p>
<div class="cplusplus"><pre>
<span class="keyword">void</span> Count_Robots_Background_Threads()
{
	int32 Count = 0;
	TRACE_CPUPROFILER_EVENT_SCOPE(Count_Robots_Background_Threads)
	{
		constexpr <span class="keyword">int</span> NumTasks = 4;
		std::<span class="keyword">array</span>&lt;int32, NumTasks&gt; Counts;
		std::fill(Counts.begin(), Counts.end(), 0);
		std::<span class="keyword">array</span>&lt; TUniquePtr&lt; FAsyncTask&lt;FCountActiveRobotsTask&gt; &gt;, NumTasks &gt; Tasks;

		int32 StepSize = Robots.Actives.Num() / NumTasks;

		<span class="comment">// create</span>
		<span class="keyword">for</span> (<span class="keyword">int</span> i = 0; i &lt; NumTasks; ++i)
		{
			Tasks[i] = 
			  MakeUnique&lt;FAsyncTask&lt;FCountActiveRobotsTask&gt;&gt;(StepSize * i, StepSize, Counts[i]);
		}
		<span class="comment">// launch</span>
		<span class="keyword">for</span> (<span class="keyword">int</span> i = 0; i &lt; NumTasks; ++i)
		{
			Tasks[i]-&gt;StartBackgroundTask();
		}
		<span class="comment">// wait</span>
		<span class="keyword">for</span> (<span class="keyword">int</span> i = 0; i &lt; NumTasks; ++i)
		{
			Tasks[i]-&gt;EnsureCompletion(<span class="keyword">true</span>, <span class="keyword">true</span>);
		}

		Count = std::accumulate(Counts.begin(), Counts.end(), 0);
	}
}
</pre></div>
<p>In this code we:</p>
<ul>
<li> allocate 4 background task objects which will run on background threads</li>
<li> divide the array of robots into 4 sections and pass each task the start and length of their section</li>
<li> allocate an array of 4 int32 objects, each task accumulates their count into one element of this array</li>
<li> after all the threads are finished we accumulate the 4 count objects into one value</li>
</ul>
<p>Now we get these timings:</p>
<p><img src="/images/mass_001_009.PNG" alt="" /></p>
<p>Running on four background threads takes 4 times longer than running on one.  What is happening
here is an issue of cache coherency and false sharing.  </p>
<p>Here is the diagram of the cache layout on a Ryzen 8 core CPU again:</p>
<p><img src="/images/Zen3_arch_20.jpg" alt="" /></p>
<p>We see that each core has its own L1 and L2 cache.  When a value is in main memory
each CPU core that refers to that value has its own copy of that value in its L1 cache.  Cache
coherency is the mechanism by which changes to these cached values are synchronized - typically
by using a protocol like <a href="https://en.wikipedia.org/wiki/MESI_protocol">MISE</a>.  </p>
<p>When one core changes a value in its cache, that value becomes invalid in every other core's cache which holds that value, and
the next time one of those other cores accesses that value it is copied from the cache of the core
which changed it.  This mechanism make sure all the cores only ever see the same
value, which make programming much easier, but it does mean that operations on the second core are stalled while value is
retrieved from the core which changed it.</p>
<p>The MISE protocol which synchronizes values between cores does not work by copying individual
values between core - it copies whole cache lines at a time.  </p>
<p>Why we see a problem here, causing the multithreaded implementation to be so much slower, is not that
we are updating the same value, but that the each thread is updating a value 
in the Counts array and all those values are on the same cache line.  This problem 
is referred to as &quot;false sharing&quot; because  while the threads are not actually sharing
count values, synchronizing the core caches using whole cache lines makes it appear as if they were.</p>
<p>This problem can be fixed in a number of ways:</p>
<ul>
<li> we could give each task its own Count value to accumulate to accumulate into and only
update the Count array once at the end</li>
<li> we could make the Count array 16 times as large and have each thread
use every 16th value - this would ensure that the used values are more 64 bytes apart
so they are not on the cache line</li>
<li> we could accumulate the values into property in a structure which was padded to be
64 bytes long, so that the values are not on the same cache line, like:</li>
</ul>
<div class="cplusplus"><pre>
<span class="keyword">struct</span> PaddedValue
{
	int32 Value = 0;
	<span class="keyword">char</span> Padding[ 64-<span class="keyword">sizeof</span>(int32)];
};
</pre></div>
<ul>
<li> we could accumulate the values into property in a structure which was aligned
to 64 bytes long, so that the values are not on the same cache line, like:</li>
</ul>
<div class="cplusplus"><pre>
<span class="keyword">struct</span> alignas(64) AlignedValue
{
	int32 Value = 0;
};
</pre></div>
<p>The code to accumulate into a local variable looks like this:</p>
<div class="cplusplus"><pre>
<span class="keyword">class</span> FCountActiveRobotsTaskLocalCount : <span class="keyword">public</span> FNonAbandonableTask
{
<span class="keyword">public</span>:
	FCountActiveRobotsTaskLocalCount(<span class="keyword">const</span> int32 InStart, <span class="keyword">const</span> int32 InNum, int32&amp; InCount)
		: Start(InStart), Num(InNum), Count(InCount)
	{
	}

	<span class="keyword">void</span> DoWork()
	{
		int32 LocalCount = 0;
		<span class="keyword">for</span> (<span class="keyword">int</span> i = Start; i &lt; Start + Num; ++i)
		{
			<span class="keyword">if</span> (Robots.Actives[i])
			{
				LocalCount++;
			}
		}
		Count = LocalCount;
	}

	TStatId GetStatId() <span class="keyword">const</span> { <span class="keyword">return</span> TStatId(); }
<span class="keyword">private</span>:
	int32 Start;
	int32 Num;
	int32&amp; Count;
};
</pre></div>
<p>and the code to pad the Counts array and only use every 16th element looks like this:</p>
<div class="cplusplus"><pre>
<span class="keyword">void</span> Count_Robots_Background_Threads_Padding()
{
	int32 Count = 0;
	TRACE_CPUPROFILER_EVENT_SCOPE(Count_Robots_Background_Threads_Padding)
	{
		constexpr <span class="keyword">int</span> NumTasks = 4;
		std::<span class="keyword">array</span>&lt;int32, NumTasks * 16 &gt; Counts;
		std::fill(Counts.begin(), Counts.end(), 0);
		std::<span class="keyword">array</span>&lt; TUniquePtr&lt; FAsyncTask&lt;FCountActiveRobotsTask&gt; &gt;, NumTasks &gt; Tasks;

		int32 StepSize = Robots.Actives.Num() / NumTasks;

		<span class="comment">// create</span>
		<span class="keyword">for</span> (<span class="keyword">int</span> i = 0; i &lt; NumTasks; ++i)
		{
			Tasks[i] =
			 MakeUnique&lt;FAsyncTask&lt;FCountActiveRobotsTask&gt;&gt;(StepSize*i, StepSize, Counts[i*16]);
		}
		<span class="comment">// launch</span>
		<span class="keyword">for</span> (<span class="keyword">int</span> i = 0; i &lt; NumTasks; ++i)
		{
			Tasks[i]-&gt;StartBackgroundTask();
		}
		<span class="comment">// wait</span>
		<span class="keyword">for</span> (<span class="keyword">int</span> i = 0; i &lt; NumTasks; ++i)
		{
			Tasks[i]-&gt;EnsureCompletion(<span class="keyword">true</span>, <span class="keyword">true</span>);
		}
		Count = std::accumulate(Counts.begin(), Counts.end(), 0);
	}
}
</pre></div>
<p>This picture shows the results:</p>
<p><img src="/images/mass_001_010.PNG" alt="" /></p>
<p>We can see that either using local variables or using padding eliminates the false sharing
and reduces the time from 1,300ms to 140ms.</p>
<h2 id="references">References</h2>
<p><a href="https://www.youtube.com/watch?v=rX0ItVEVjHc">Mike Acton &quot;Data-Oriented Design and C++&quot;</a>  <br />
<a href="https://learn.microsoft.com/en-us/sysinternals/downloads/coreinfo">CoreInfo</a>      <br />
<a href="https://www.youtube.com/watch?v=Nz9SiF0QVKY">Writing Cache Friendly C++</a>  <br />
<a href="https://www.youtube.com/watch?v=BP6NxVxDQIs">Want Fast C++</a> </p>

</div>
<footer class="no-sidebar">
<div>
<p class="page-footer">
MIT Licensed | Copyright © 2020-2024 John Farrow
: john.farrow@unrealcode.net
</p>
</div>
</footer>
</main>
</div>
</body>
</html>
